{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "TOP_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if TOP_DIR not in sys.path:\n",
    "    sys.path.insert(0, TOP_DIR)\n",
    "from causalign.constants import CAUSALIGN_DIR, CITING_ID_COL, CITED_ID_COL, NEGATIVE_ID_COL, CORPUS_ID_COL\n",
    "from causalign.data.utils import load_imdb_data, load_civil_commments_data\n",
    "from causalign.data.generators import IMDBDataset, CivilCommentsDataset\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from causalign.utils import save_model, get_default_training_args, seed_everything\n",
    "from pprint import pprint\n",
    "seed_everything(328)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyperparameters + combine with default args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting dataset for sentiment task (IMDB or CivilComments)...\n",
      "Setting hyperparameters for sentiment task...\n"
     ]
    }
   ],
   "source": [
    "sys.argv = [\n",
    "    'notebook',\n",
    "    '--limit_data', '500',\n",
    "    '--max_seq_length', '150',\n",
    "    '--lr', '5e-5',\n",
    "    '--treatment_phrase', 'love',\n",
    "    '--lambda_bce', '1.0',\n",
    "    '--lambda_reg', '0.001',\n",
    "    '--lambda_riesz', '0.01',\n",
    "    '--dataset', 'imdb',\n",
    "    '--log_every', '5',\n",
    "    #'--running_ate',\n",
    "    #'--estimate_targets_for_ate',\n",
    "]\n",
    "args = get_default_training_args(regime = 'base_sent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limiting data to 500 rows.\n",
      "Creating treated and control counterfactuals...\n",
      "Tokenizing texts for real, treated, and control counterfactuals...\n",
      "Truncating texts during tokenization to length: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing texts: 100%|██████████| 500/500 [00:00<00:00, 464588.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncating texts during tokenization to length: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing texts: 100%|██████████| 500/500 [00:00<00:00, 599528.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncating texts during tokenization to length: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing texts: 100%|██████████| 500/500 [00:00<00:00, 496955.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limiting data to 500 rows.\n",
      "Tokenizing texts for real, treated, and control counterfactuals...\n",
      "Truncating texts during tokenization to length: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing texts: 100%|██████████| 500/500 [00:00<00:00, 662397.98it/s]\n"
     ]
    }
   ],
   "source": [
    "if args.dataset == \"imdb\":\n",
    "    imdb_train = load_imdb_data(split = \"train\")\n",
    "    imdb_val = load_imdb_data(split = \"test\")\n",
    "\n",
    "    imdb_ds_train: IMDBDataset = IMDBDataset(imdb_train, \n",
    "                                    split=\"train\",\n",
    "                                    args=args)\n",
    "    imdb_ds_val: IMDBDataset = IMDBDataset(imdb_val,\n",
    "                                        split = \"validation\", \n",
    "                                        args = args)\n",
    "    ds_train = imdb_ds_train\n",
    "    ds_val = imdb_ds_val\n",
    "else: \n",
    "    civil_train = load_civil_commments_data(split = \"train\")\n",
    "    civil_val = load_civil_commments_data(split = \"test\")\n",
    "    \n",
    "    civil_ds_train: CivilCommentsDataset = CivilCommentsDataset(civil_train, \n",
    "                                                split=\"train\",\n",
    "                                                args=args)\n",
    "    civil_ds_val: CivilCommentsDataset = CivilCommentsDataset(civil_val,\n",
    "                                                split = \"validation\", \n",
    "                                                args = args)\n",
    "    ds_train = civil_ds_train\n",
    "    ds_val = civil_ds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Train Dataset size: {len(ds_train)}\")\n",
    "# print(\"Example train data point:\")\n",
    "# example = ds_train[0]\n",
    "# pprint(example)\n",
    "# \n",
    "# print() \n",
    "# \n",
    "# print(f\"Val Dataset size: {len(ds_val)}\")\n",
    "# print(\"Example val data point:\")\n",
    "# example = ds_val[0]\n",
    "# pprint(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets = [d['target'] for d in ds_train]\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.hist(targets, bins=20, edgecolor='black', alpha=0.7)\n",
    "# plt.title(\"Histogram of Target Values in Train Dataset\")\n",
    "# plt.xlabel(\"Target Value\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop a Training Regime for CausalSent sentiment analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP_DIR: /Users/danielfrees/Desktop/causalign\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:df677pv7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>Val Accuracy</td><td>▄▇▇▁█▇▆▅▃▃▃▅▆▇▇▇▇▇▇▆</td></tr><tr><td>Val F1</td><td>▃▇█▁██▅▄▃▂▂▄▆▇▇▇▇▇▇▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>20</td></tr><tr><td>Val Accuracy</td><td>0.71</td></tr><tr><td>Val F1</td><td>0.72897</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">solar-dream-26</strong> at: <a href='https://wandb.ai/danielfrees-google/causal-sentiment/runs/df677pv7' target=\"_blank\">https://wandb.ai/danielfrees-google/causal-sentiment/runs/df677pv7</a><br/> View project at: <a href='https://wandb.ai/danielfrees-google/causal-sentiment' target=\"_blank\">https://wandb.ai/danielfrees-google/causal-sentiment</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241123_185422-df677pv7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:df677pv7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/danielfrees/Desktop/causalign/notebooks/wandb/run-20241123_185553-0oehfqv5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/danielfrees-google/causal-sentiment/runs/0oehfqv5' target=\"_blank\">dandy-thunder-27</a></strong> to <a href='https://wandb.ai/danielfrees-google/causal-sentiment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/danielfrees-google/causal-sentiment' target=\"_blank\">https://wandb.ai/danielfrees-google/causal-sentiment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/danielfrees-google/causal-sentiment/runs/0oehfqv5' target=\"_blank\">https://wandb.ai/danielfrees-google/causal-sentiment/runs/0oehfqv5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Epoch 1/20, Batch 5/16, Loss: 0.7093, Accuracy: 0.4750, F1: 0.2500, Tau_Hat_love: 0.0188\n",
      "Epoch 1/20, Batch 10/16, Loss: 0.7459, Accuracy: 0.5000, F1: 0.5322, Tau_Hat_love: -0.0440\n",
      "Epoch 1/20, Batch 15/16, Loss: 0.6537, Accuracy: 0.5229, F1: 0.5033, Tau_Hat_love: -0.0601\n",
      "Epoch 1/20 Validation Accuracy: 0.7100, F1: 0.7140\n",
      "Epoch 2/20, Batch 5/16, Loss: 0.6207, Accuracy: 0.7438, F1: 0.8019, Tau_Hat_love: 0.0050\n",
      "Epoch 2/20, Batch 10/16, Loss: 0.4933, Accuracy: 0.7688, F1: 0.8021, Tau_Hat_love: 0.0232\n",
      "Epoch 2/20, Batch 15/16, Loss: 0.4853, Accuracy: 0.7979, F1: 0.8194, Tau_Hat_love: 0.0748\n",
      "Epoch 2/20 Validation Accuracy: 0.7820, F1: 0.7850\n",
      "Epoch 3/20, Batch 5/16, Loss: 0.1938, Accuracy: 0.8750, F1: 0.8780, Tau_Hat_love: 0.0011\n",
      "Epoch 3/20, Batch 10/16, Loss: 0.1509, Accuracy: 0.8656, F1: 0.8746, Tau_Hat_love: 0.0755\n",
      "Epoch 3/20, Batch 15/16, Loss: 0.1596, Accuracy: 0.9000, F1: 0.9020, Tau_Hat_love: 0.0064\n",
      "Epoch 3/20 Validation Accuracy: 0.7600, F1: 0.7414\n",
      "Epoch 4/20, Batch 5/16, Loss: 0.0144, Accuracy: 0.9812, F1: 0.9809, Tau_Hat_love: -0.0896\n",
      "Epoch 4/20, Batch 10/16, Loss: -0.0016, Accuracy: 0.9844, F1: 0.9850, Tau_Hat_love: -0.2454\n",
      "Epoch 4/20, Batch 15/16, Loss: -0.0341, Accuracy: 0.9896, F1: 0.9898, Tau_Hat_love: -0.2571\n",
      "Epoch 4/20 Validation Accuracy: 0.7860, F1: 0.7930\n",
      "Epoch 5/20, Batch 5/16, Loss: -0.0720, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.2515\n",
      "Epoch 5/20, Batch 10/16, Loss: -0.0718, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.2917\n",
      "Epoch 5/20, Batch 15/16, Loss: -0.0931, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3076\n",
      "Epoch 5/20 Validation Accuracy: 0.7860, F1: 0.7881\n",
      "Epoch 6/20, Batch 5/16, Loss: -0.0967, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.1990\n",
      "Epoch 6/20, Batch 10/16, Loss: -0.1038, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3640\n",
      "Epoch 6/20, Batch 15/16, Loss: -0.0916, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3359\n",
      "Epoch 6/20 Validation Accuracy: 0.8160, F1: 0.8339\n",
      "Epoch 7/20, Batch 5/16, Loss: -0.1277, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3970\n",
      "Epoch 7/20, Batch 10/16, Loss: -0.1447, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3545\n",
      "Epoch 7/20, Batch 15/16, Loss: -0.1246, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.2237\n",
      "Epoch 7/20 Validation Accuracy: 0.8000, F1: 0.8039\n",
      "Epoch 8/20, Batch 5/16, Loss: -0.1250, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.4116\n",
      "Epoch 8/20, Batch 10/16, Loss: -0.1293, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3288\n",
      "Epoch 8/20, Batch 15/16, Loss: -0.1518, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3624\n",
      "Epoch 8/20 Validation Accuracy: 0.8080, F1: 0.8267\n",
      "Epoch 9/20, Batch 5/16, Loss: -0.1556, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3395\n",
      "Epoch 9/20, Batch 10/16, Loss: -0.1706, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3408\n",
      "Epoch 9/20, Batch 15/16, Loss: -0.1436, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.4004\n",
      "Epoch 9/20 Validation Accuracy: 0.7800, F1: 0.7791\n",
      "Epoch 10/20, Batch 5/16, Loss: -0.1680, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.4422\n",
      "Epoch 10/20, Batch 10/16, Loss: -0.1559, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3228\n",
      "Epoch 10/20, Batch 15/16, Loss: -0.1854, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.4232\n",
      "Epoch 10/20 Validation Accuracy: 0.8080, F1: 0.8267\n",
      "Epoch 11/20, Batch 5/16, Loss: -0.1701, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.2395\n",
      "Epoch 11/20, Batch 10/16, Loss: -0.1840, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.2620\n",
      "Epoch 11/20, Batch 15/16, Loss: -0.1899, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.4568\n",
      "Epoch 11/20 Validation Accuracy: 0.7980, F1: 0.8140\n",
      "Epoch 12/20, Batch 5/16, Loss: -0.1756, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.4888\n",
      "Epoch 12/20, Batch 10/16, Loss: -0.1661, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3133\n",
      "Epoch 12/20, Batch 15/16, Loss: -0.1902, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.1460\n",
      "Epoch 12/20 Validation Accuracy: 0.8100, F1: 0.8224\n",
      "Epoch 13/20, Batch 5/16, Loss: -0.1836, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.7399\n",
      "Epoch 13/20, Batch 10/16, Loss: -0.1963, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3854\n",
      "Epoch 13/20, Batch 15/16, Loss: -0.1837, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.2487\n",
      "Epoch 13/20 Validation Accuracy: 0.8020, F1: 0.8190\n",
      "Epoch 14/20, Batch 5/16, Loss: -0.1543, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3274\n",
      "Epoch 14/20, Batch 10/16, Loss: -0.1938, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.1911\n",
      "Epoch 14/20, Batch 15/16, Loss: -0.1952, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3949\n",
      "Epoch 14/20 Validation Accuracy: 0.7980, F1: 0.8076\n",
      "Epoch 15/20, Batch 5/16, Loss: -0.1575, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.5015\n",
      "Epoch 15/20, Batch 10/16, Loss: -0.1389, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.2943\n",
      "Epoch 15/20, Batch 15/16, Loss: -0.1817, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.2722\n",
      "Epoch 15/20 Validation Accuracy: 0.7980, F1: 0.8076\n",
      "Epoch 16/20, Batch 5/16, Loss: -0.2150, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.2202\n",
      "Epoch 16/20, Batch 10/16, Loss: -0.2118, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.4130\n",
      "Epoch 16/20, Batch 15/16, Loss: -0.1611, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3619\n",
      "Epoch 16/20 Validation Accuracy: 0.8040, F1: 0.8130\n",
      "Epoch 17/20, Batch 5/16, Loss: -0.2080, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3219\n",
      "Epoch 17/20, Batch 10/16, Loss: -0.2157, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.4781\n",
      "Epoch 17/20, Batch 15/16, Loss: -0.2261, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.5153\n",
      "Epoch 17/20 Validation Accuracy: 0.8080, F1: 0.8175\n",
      "Epoch 18/20, Batch 5/16, Loss: -0.2284, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3556\n",
      "Epoch 18/20, Batch 10/16, Loss: -0.2044, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.2908\n",
      "Epoch 18/20, Batch 15/16, Loss: -0.2075, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.1795\n",
      "Epoch 18/20 Validation Accuracy: 0.7980, F1: 0.8031\n",
      "Epoch 19/20, Batch 5/16, Loss: -0.2132, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.2918\n",
      "Epoch 19/20, Batch 10/16, Loss: -0.1994, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3499\n",
      "Epoch 19/20, Batch 15/16, Loss: -0.2525, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.4952\n",
      "Epoch 19/20 Validation Accuracy: 0.8000, F1: 0.8069\n",
      "Epoch 20/20, Batch 5/16, Loss: -0.2013, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3312\n",
      "Epoch 20/20, Batch 10/16, Loss: -0.2240, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3239\n",
      "Epoch 20/20, Batch 15/16, Loss: -0.2426, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.4007\n",
      "Epoch 20/20 Validation Accuracy: 0.8060, F1: 0.8152\n",
      "Training complete! :)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "TOP_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "print(\"TOP_DIR:\", TOP_DIR)\n",
    "if TOP_DIR not in sys.path:\n",
    "    sys.path.insert(0, TOP_DIR)\n",
    "from causalign.modules.causal_sent import CausalSent\n",
    "from causalign.data.generators import SimilarityDataset\n",
    "import wandb\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project=\"causal-sentiment\", config=args)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                    else \"mps\" if torch.backends.mps.is_available() \n",
    "                    else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "lambda_bce: float = args.lambda_bce\n",
    "lambda_reg: float = args.lambda_reg\n",
    "lambda_riesz: float = args.lambda_riesz\n",
    "batch_size: int = args.batch_size\n",
    "epochs: int = args.epochs\n",
    "log_every: int = args.log_every\n",
    "running_ate: bool = args.running_ate # whether to track a running average or batch average to compute the RR ATE\n",
    "pretrained_model_name: str = args.pretrained_model_name\n",
    "lr: float = args.lr\n",
    "estimate_targets_for_ate: bool = args.estimate_targets_for_ate # whether to use estimated sentiment probabilities or true targets to compute the RR ATE\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=True, collate_fn=SimilarityDataset.collate_fn)\n",
    "val_loader = DataLoader(ds_val, batch_size=batch_size, collate_fn=SimilarityDataset.collate_fn)\n",
    "\n",
    "# Model, optimizer, and loss\n",
    "model = CausalSent(bert_hidden_size=768, pretrained_model_name=pretrained_model_name).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    train_targets, train_predictions = [], []\n",
    "    \n",
    "    for i, batch in enumerate(train_loader):\n",
    "        input_ids_real = batch['input_ids_real'].to(device)\n",
    "        input_ids_treated = batch['input_ids_treated'].to(device)\n",
    "        input_ids_control = batch['input_ids_control'].to(device)\n",
    "        attention_mask_real = batch['attention_mask_real'].to(device)\n",
    "        attention_mask_treated = batch['attention_mask_treated'].to(device)\n",
    "        attention_mask_control = batch['attention_mask_control'].to(device)\n",
    "        targets = batch['targets'].float().to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        (sentiment_outputs_real, sentiment_outputs_treated, sentiment_outputs_control, \n",
    "        riesz_outputs_real, riesz_outputs_treated, riesz_outputs_control) = model(\n",
    "            input_ids_real,\n",
    "            input_ids_treated,\n",
    "            input_ids_control,\n",
    "            attention_mask_real,\n",
    "            attention_mask_treated,\n",
    "            attention_mask_control,\n",
    "        )\n",
    "\n",
    "        # Compute tau_hat\n",
    "        if running_ate:\n",
    "            if \"epoch_riesz_outputs\" not in locals():\n",
    "                epoch_riesz_outputs, epoch_sentiment_outputs, epoch_targets = [], [], []\n",
    "            epoch_riesz_outputs.append(riesz_outputs_real.detach())\n",
    "            epoch_sentiment_outputs.append(torch.sigmoid(sentiment_outputs_real.detach()))\n",
    "            epoch_targets.append(targets.detach())\n",
    "\n",
    "            all_riesz_outputs = torch.cat(epoch_riesz_outputs, dim=0)\n",
    "            all_sentiment_outputs = torch.cat(epoch_sentiment_outputs, dim=0)\n",
    "            all_targets = torch.cat(epoch_targets, dim=0)\n",
    "\n",
    "            tau_hat = torch.mean(all_riesz_outputs * all_sentiment_outputs if estimate_targets_for_ate else all_targets)\n",
    "        else:\n",
    "            tau_hat = torch.mean(riesz_outputs_real * (torch.sigmoid(sentiment_outputs_real) if estimate_targets_for_ate else targets))\n",
    "        \n",
    "        # Compute losses\n",
    "        riesz_loss = torch.mean(-2 * (riesz_outputs_treated - riesz_outputs_control) + (riesz_outputs_real ** 2))\n",
    "        reg_loss = torch.mean((sentiment_outputs_treated - sentiment_outputs_control - tau_hat) ** 2)\n",
    "        bce = bce_loss(sentiment_outputs_real.squeeze(), targets)\n",
    "        loss = lambda_bce * bce + lambda_reg * reg_loss + lambda_riesz * riesz_loss\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Training metrics\n",
    "        preds = torch.sigmoid(sentiment_outputs_real).squeeze().detach().cpu().numpy()\n",
    "        preds = (preds > 0.5).astype(int)\n",
    "        train_targets.extend(targets.cpu().numpy())\n",
    "        train_predictions.extend(preds)\n",
    "\n",
    "        # Logging\n",
    "        if (i + 1) % log_every == 0:\n",
    "            train_acc = accuracy_score(train_targets, train_predictions)\n",
    "            train_f1 = f1_score(train_targets, train_predictions)\n",
    "            wandb.log({\"Train Loss\": loss.item(), \n",
    "                    \"Train Accuracy\": train_acc, \n",
    "                    \"Train F1\": train_f1, \n",
    "                    f\"Tau_Hat_{args.treatment_phrase}\": tau_hat.item(),\n",
    "                    \"Batch\": i + 1})\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{epochs}, \"\n",
    "                f\"Batch {i + 1}/{len(train_loader)}, \"\n",
    "                f\"Loss: {loss.item():.4f}, \"\n",
    "                f\"Accuracy: {train_acc:.4f}, \"\n",
    "                f\"F1: {train_f1:.4f}, \"\n",
    "                f\"Tau_Hat_{args.treatment_phrase}: {tau_hat.item():.4f}\"\n",
    "            )\n",
    "    # ======= Validation Metrics =======\n",
    "    model.eval()\n",
    "    val_targets, val_predictions = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids_real = batch['input_ids_real'].to(device)\n",
    "            attention_mask_real = batch['attention_mask_real'].to(device)\n",
    "            targets = batch['targets'].float().to(device)\n",
    "            \n",
    "            sentiment_output_real = model(input_ids_real, None, None, attention_mask_real, None, None)\n",
    "            preds = torch.sigmoid(sentiment_output_real).squeeze().cpu().numpy()\n",
    "            preds = (preds > 0.5).astype(int)\n",
    "            \n",
    "            val_targets.extend(targets.cpu().numpy())\n",
    "            val_predictions.extend(preds)\n",
    "    \n",
    "    # Compute validation metrics\n",
    "    val_acc = accuracy_score(val_targets, val_predictions)\n",
    "    val_f1 = f1_score(val_targets, val_predictions)\n",
    "    wandb.log({\"Val Accuracy\": val_acc, \"Val F1\": val_f1, \"Epoch\": epoch + 1})\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} Validation Accuracy: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
    "\n",
    "print(\"Training complete! :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causalign",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
