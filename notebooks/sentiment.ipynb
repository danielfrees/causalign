{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "TOP_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if TOP_DIR not in sys.path:\n",
    "    sys.path.insert(0, TOP_DIR)\n",
    "from causalign.constants import CAUSALIGN_DIR, CITING_ID_COL, CITED_ID_COL, NEGATIVE_ID_COL, CORPUS_ID_COL\n",
    "from causalign.data.utils import load_imdb_data, load_civil_comments_data\n",
    "from causalign.data.generators import IMDBDataset, CivilCommentsDataset\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from causalign.utils import save_model, get_default_sent_training_args, seed_everything\n",
    "from pprint import pprint\n",
    "seed_everything(328)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyperparameters + combine with default args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting dataset for sentiment task (IMDB or CivilComments)...\n",
      "Setting hyperparameters for sentiment task...\n"
     ]
    }
   ],
   "source": [
    "sys.argv = [\n",
    "    'notebook',\n",
    "    '--limit_data', '500',\n",
    "    '--max_seq_length', '150',\n",
    "    '--lr', '5e-5',\n",
    "    '--treatment_phrase', 'love',\n",
    "    '--lambda_bce', '1.0',\n",
    "    '--lambda_reg', '0.001',\n",
    "    '--lambda_riesz', '0.01',\n",
    "    '--dataset', 'imdb',\n",
    "    '--log_every', '5',\n",
    "    #'--running_ate',\n",
    "    #'--estimate_targets_for_ate',\n",
    "]\n",
    "args = get_default_sent_training_args(regime = 'causal_sent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train_original = load_imdb_data(split = \"train\")\n",
    "imdb_train_splits = imdb_train_original.train_test_split(test_size=0.2)\n",
    "imdb_train = imdb_train_splits[\"train\"]\n",
    "imdb_val = imdb_train_splits[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Normally I try to avoid Sci-Fi movies as much as I can, because this just isn\\'t a genre that really appeals to me. Light sabers, UFO\\'s, aliens, time traveling... most of the time it\\'s nothing for me. However, there is one movie in the genre that I\\'ll always give a place in my list of top movies and that\\'s this \"Twelve Monkeys\" I remember to be completely blown away by it the first time, but even now, after having it seen several times already, I\\'m still one of its biggest fans. Every time I see it, this movie seems to get better and better.<br /><br />Somewhere in the distant future all people live underground because an unknown and lethal virus wiped out five billion people in 1996, leaving only 1 percent of the population alive. James Cole is one of them. He\\'s a prisoner who lives in a small cage and who is chosen as a \\'volunteer\\' to be sent back to in time to gather information about the origin of the epidemic. They believe it was spread by a mysterious group called \\'The Twelve Monkeys\\' and need the virus before it mutated, so that scientists can study it. But their time traveling machine doesn\\'t work perfectly yet and he is accidentally sent to 1990, where he meets Dr. Kathryn Railly, a psychiatrist, and Jeffrey Goines, the insane son of a famous scientist and virus expert...<br /><br />What I like so much about this movie is the fact that it is never clear whether all what you are seeing is real or not. Is this just an illusion, created in the mind of a mentally ill man or is it real? Does he really come from the future and can he really travel through time? Was the population really wiped out by a virus, released by the army of The Twelve Monkeys? Those are all questions that will leave you wondering from the beginning until the end. If the makers of this movie had chosen to make it all more obvious, I\\'m sure that I would never have liked it as much as I did now. It\\'s just that mysteriousness that keeps me interested time after time. But that\\'s not the only good thing about this movie of course. The acting is amazing too. Normally I\\'m not too much a fan of Bruce Willis, but what he did in this movie was just astonishing. Together with Madeleine Stowe and Brad Pitt he should have won several awards for it, because together with the amazing story, they made this movie work so incredibly well.<br /><br />Even after several viewings, I\\'m still a huge fan of this movie. Except for this movie, I have only seen one other Terry Gilliam movie and that\\'s \"Fear and Loathing in Las Vegas\", which wasn\\'t bad, but didn\\'t really convince me either. However, it\\'s this movie that really makes me look forward to his other work. I give it a 9/10, maybe even a 9.5/10.',\n",
       " 'I recently found a copy for $5 at a video store, and snapped it up eagerly. While the music and (obviously) graphics aren\\'t up to the standards of my favorite of the series, Beyond the Mind\\'s Eye, I am still entranced by one segment:<br /><br />Stanley and Stella in \"Breaking the Ice\". The music is brilliant, and the emotions feel real. The clip on Odyssey\\'s website doesn\\'t have the story nor the music, unfortunately.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_train['text'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in imdb_train['text'] if not x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_train_original['text'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 20000\n",
      "})\n",
      "Limiting data to 500 rows.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Model sentence-transformers/msmarco-distilbert-base-v4 not supported. Tokenizer could not be initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(imdb_train))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(imdb_train)\n\u001b[0;32m---> 10\u001b[0m imdb_ds_train: IMDBDataset \u001b[38;5;241m=\u001b[39m \u001b[43mIMDBDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimdb_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m imdb_ds_val: IMDBDataset \u001b[38;5;241m=\u001b[39m IMDBDataset(imdb_val,\n\u001b[1;32m     14\u001b[0m                                     split \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     15\u001b[0m                                     args \u001b[38;5;241m=\u001b[39m args)\n\u001b[1;32m     16\u001b[0m ds_train \u001b[38;5;241m=\u001b[39m imdb_ds_train\n",
      "File \u001b[0;32m~/Desktop/causalign/causalign/data/generators.py:279\u001b[0m, in \u001b[0;36mIMDBDataset.__init__\u001b[0;34m(self, dataset, split, args)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m    276\u001b[0m             dataset: Dataset, \n\u001b[1;32m    277\u001b[0m             split: \u001b[38;5;28mstr\u001b[39m, \n\u001b[1;32m    278\u001b[0m             args):\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_col\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_col\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/causalign/causalign/data/generators.py:118\u001b[0m, in \u001b[0;36mSimilarityDataset.__init__\u001b[0;34m(self, dataset, split, args, text_col, label_col)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m DistilBertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(args\u001b[38;5;241m.\u001b[39mpretrained_model_name, token \u001b[38;5;241m=\u001b[39m HF_TOKEN)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mpretrained_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not supported. Tokenizer could not be initialized.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# ======== Create treated and control counterfactuals for each example ========\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtreat_if_untreated\u001b[39m(\n\u001b[1;32m    123\u001b[0m         text: \u001b[38;5;28mstr\u001b[39m, \n\u001b[1;32m    124\u001b[0m         treatment_phrase: \u001b[38;5;28mstr\u001b[39m, \n\u001b[1;32m    125\u001b[0m         append_where: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m    126\u001b[0m         ignore_case: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[0;31mValueError\u001b[0m: Model sentence-transformers/msmarco-distilbert-base-v4 not supported. Tokenizer could not be initialized."
     ]
    }
   ],
   "source": [
    "if args.dataset == \"imdb\":\n",
    "    imdb_train_original = load_imdb_data(split = \"train\")\n",
    "    imdb_train_splits = imdb_train_original.train_test_split(test_size=0.2)\n",
    "    imdb_train = imdb_train_splits[\"train\"]\n",
    "    imdb_val = imdb_train_splits[\"test\"]\n",
    "    \n",
    "    print(type(imdb_train))\n",
    "    print(imdb_train)\n",
    "\n",
    "    imdb_ds_train: IMDBDataset = IMDBDataset(imdb_train, \n",
    "                                    split=\"train\",\n",
    "                                    args=args)\n",
    "    imdb_ds_val: IMDBDataset = IMDBDataset(imdb_val,\n",
    "                                        split = \"validation\", \n",
    "                                        args = args)\n",
    "    ds_train = imdb_ds_train\n",
    "    ds_val = imdb_ds_val\n",
    "else: \n",
    "    civil_train = load_civil_comments_data(split = \"train\")\n",
    "    civil_val = load_civil_comments_data(split = \"test\")\n",
    "    \n",
    "    civil_ds_train: CivilCommentsDataset = CivilCommentsDataset(civil_train, \n",
    "                                                split=\"train\",\n",
    "                                                args=args)\n",
    "    civil_ds_val: CivilCommentsDataset = CivilCommentsDataset(civil_val,\n",
    "                                                split = \"validation\", \n",
    "                                                args = args)\n",
    "    ds_train = civil_ds_train\n",
    "    ds_val = civil_ds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Train Dataset size: {len(ds_train)}\")\n",
    "# print(\"Example train data point:\")\n",
    "# example = ds_train[0]\n",
    "# pprint(example)\n",
    "# \n",
    "# print() \n",
    "# \n",
    "# print(f\"Val Dataset size: {len(ds_val)}\")\n",
    "# print(\"Example val data point:\")\n",
    "# example = ds_val[0]\n",
    "# pprint(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets = [d['target'] for d in ds_train]\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.hist(targets, bins=20, edgecolor='black', alpha=0.7)\n",
    "# plt.title(\"Histogram of Target Values in Train Dataset\")\n",
    "# plt.xlabel(\"Target Value\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop a Training Regime for CausalSent sentiment analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP_DIR: /Users/danielfrees/Desktop/causalign\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:df677pv7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>Val Accuracy</td><td>▄▇▇▁█▇▆▅▃▃▃▅▆▇▇▇▇▇▇▆</td></tr><tr><td>Val F1</td><td>▃▇█▁██▅▄▃▂▂▄▆▇▇▇▇▇▇▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>20</td></tr><tr><td>Val Accuracy</td><td>0.71</td></tr><tr><td>Val F1</td><td>0.72897</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">solar-dream-26</strong> at: <a href='https://wandb.ai/danielfrees-google/causal-sentiment/runs/df677pv7' target=\"_blank\">https://wandb.ai/danielfrees-google/causal-sentiment/runs/df677pv7</a><br/> View project at: <a href='https://wandb.ai/danielfrees-google/causal-sentiment' target=\"_blank\">https://wandb.ai/danielfrees-google/causal-sentiment</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241123_185422-df677pv7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:df677pv7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/danielfrees/Desktop/causalign/notebooks/wandb/run-20241123_185553-0oehfqv5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/danielfrees-google/causal-sentiment/runs/0oehfqv5' target=\"_blank\">dandy-thunder-27</a></strong> to <a href='https://wandb.ai/danielfrees-google/causal-sentiment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/danielfrees-google/causal-sentiment' target=\"_blank\">https://wandb.ai/danielfrees-google/causal-sentiment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/danielfrees-google/causal-sentiment/runs/0oehfqv5' target=\"_blank\">https://wandb.ai/danielfrees-google/causal-sentiment/runs/0oehfqv5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Epoch 1/20, Batch 5/16, Loss: 0.7093, Accuracy: 0.4750, F1: 0.2500, Tau_Hat_love: 0.0188\n",
      "Epoch 1/20, Batch 10/16, Loss: 0.7459, Accuracy: 0.5000, F1: 0.5322, Tau_Hat_love: -0.0440\n",
      "Epoch 1/20, Batch 15/16, Loss: 0.6537, Accuracy: 0.5229, F1: 0.5033, Tau_Hat_love: -0.0601\n",
      "Epoch 1/20 Validation Accuracy: 0.7100, F1: 0.7140\n",
      "Epoch 2/20, Batch 5/16, Loss: 0.6207, Accuracy: 0.7438, F1: 0.8019, Tau_Hat_love: 0.0050\n",
      "Epoch 2/20, Batch 10/16, Loss: 0.4933, Accuracy: 0.7688, F1: 0.8021, Tau_Hat_love: 0.0232\n",
      "Epoch 2/20, Batch 15/16, Loss: 0.4853, Accuracy: 0.7979, F1: 0.8194, Tau_Hat_love: 0.0748\n",
      "Epoch 2/20 Validation Accuracy: 0.7820, F1: 0.7850\n",
      "Epoch 3/20, Batch 5/16, Loss: 0.1938, Accuracy: 0.8750, F1: 0.8780, Tau_Hat_love: 0.0011\n",
      "Epoch 3/20, Batch 10/16, Loss: 0.1509, Accuracy: 0.8656, F1: 0.8746, Tau_Hat_love: 0.0755\n",
      "Epoch 3/20, Batch 15/16, Loss: 0.1596, Accuracy: 0.9000, F1: 0.9020, Tau_Hat_love: 0.0064\n",
      "Epoch 3/20 Validation Accuracy: 0.7600, F1: 0.7414\n",
      "Epoch 4/20, Batch 5/16, Loss: 0.0144, Accuracy: 0.9812, F1: 0.9809, Tau_Hat_love: -0.0896\n",
      "Epoch 4/20, Batch 10/16, Loss: -0.0016, Accuracy: 0.9844, F1: 0.9850, Tau_Hat_love: -0.2454\n",
      "Epoch 4/20, Batch 15/16, Loss: -0.0341, Accuracy: 0.9896, F1: 0.9898, Tau_Hat_love: -0.2571\n",
      "Epoch 4/20 Validation Accuracy: 0.7860, F1: 0.7930\n",
      "Epoch 5/20, Batch 5/16, Loss: -0.0720, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.2515\n",
      "Epoch 5/20, Batch 10/16, Loss: -0.0718, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.2917\n",
      "Epoch 5/20, Batch 15/16, Loss: -0.0931, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3076\n",
      "Epoch 5/20 Validation Accuracy: 0.7860, F1: 0.7881\n",
      "Epoch 6/20, Batch 5/16, Loss: -0.0967, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.1990\n",
      "Epoch 6/20, Batch 10/16, Loss: -0.1038, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3640\n",
      "Epoch 6/20, Batch 15/16, Loss: -0.0916, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3359\n",
      "Epoch 6/20 Validation Accuracy: 0.8160, F1: 0.8339\n",
      "Epoch 7/20, Batch 5/16, Loss: -0.1277, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3970\n",
      "Epoch 7/20, Batch 10/16, Loss: -0.1447, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3545\n",
      "Epoch 7/20, Batch 15/16, Loss: -0.1246, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.2237\n",
      "Epoch 7/20 Validation Accuracy: 0.8000, F1: 0.8039\n",
      "Epoch 8/20, Batch 5/16, Loss: -0.1250, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.4116\n",
      "Epoch 8/20, Batch 10/16, Loss: -0.1293, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3288\n",
      "Epoch 8/20, Batch 15/16, Loss: -0.1518, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3624\n",
      "Epoch 8/20 Validation Accuracy: 0.8080, F1: 0.8267\n",
      "Epoch 9/20, Batch 5/16, Loss: -0.1556, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3395\n",
      "Epoch 9/20, Batch 10/16, Loss: -0.1706, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3408\n",
      "Epoch 9/20, Batch 15/16, Loss: -0.1436, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.4004\n",
      "Epoch 9/20 Validation Accuracy: 0.7800, F1: 0.7791\n",
      "Epoch 10/20, Batch 5/16, Loss: -0.1680, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.4422\n",
      "Epoch 10/20, Batch 10/16, Loss: -0.1559, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3228\n",
      "Epoch 10/20, Batch 15/16, Loss: -0.1854, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.4232\n",
      "Epoch 10/20 Validation Accuracy: 0.8080, F1: 0.8267\n",
      "Epoch 11/20, Batch 5/16, Loss: -0.1701, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.2395\n",
      "Epoch 11/20, Batch 10/16, Loss: -0.1840, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.2620\n",
      "Epoch 11/20, Batch 15/16, Loss: -0.1899, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.4568\n",
      "Epoch 11/20 Validation Accuracy: 0.7980, F1: 0.8140\n",
      "Epoch 12/20, Batch 5/16, Loss: -0.1756, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.4888\n",
      "Epoch 12/20, Batch 10/16, Loss: -0.1661, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3133\n",
      "Epoch 12/20, Batch 15/16, Loss: -0.1902, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.1460\n",
      "Epoch 12/20 Validation Accuracy: 0.8100, F1: 0.8224\n",
      "Epoch 13/20, Batch 5/16, Loss: -0.1836, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.7399\n",
      "Epoch 13/20, Batch 10/16, Loss: -0.1963, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3854\n",
      "Epoch 13/20, Batch 15/16, Loss: -0.1837, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.2487\n",
      "Epoch 13/20 Validation Accuracy: 0.8020, F1: 0.8190\n",
      "Epoch 14/20, Batch 5/16, Loss: -0.1543, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3274\n",
      "Epoch 14/20, Batch 10/16, Loss: -0.1938, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.1911\n",
      "Epoch 14/20, Batch 15/16, Loss: -0.1952, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3949\n",
      "Epoch 14/20 Validation Accuracy: 0.7980, F1: 0.8076\n",
      "Epoch 15/20, Batch 5/16, Loss: -0.1575, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.5015\n",
      "Epoch 15/20, Batch 10/16, Loss: -0.1389, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.2943\n",
      "Epoch 15/20, Batch 15/16, Loss: -0.1817, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.2722\n",
      "Epoch 15/20 Validation Accuracy: 0.7980, F1: 0.8076\n",
      "Epoch 16/20, Batch 5/16, Loss: -0.2150, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.2202\n",
      "Epoch 16/20, Batch 10/16, Loss: -0.2118, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.4130\n",
      "Epoch 16/20, Batch 15/16, Loss: -0.1611, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3619\n",
      "Epoch 16/20 Validation Accuracy: 0.8040, F1: 0.8130\n",
      "Epoch 17/20, Batch 5/16, Loss: -0.2080, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3219\n",
      "Epoch 17/20, Batch 10/16, Loss: -0.2157, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.4781\n",
      "Epoch 17/20, Batch 15/16, Loss: -0.2261, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.5153\n",
      "Epoch 17/20 Validation Accuracy: 0.8080, F1: 0.8175\n",
      "Epoch 18/20, Batch 5/16, Loss: -0.2284, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3556\n",
      "Epoch 18/20, Batch 10/16, Loss: -0.2044, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.2908\n",
      "Epoch 18/20, Batch 15/16, Loss: -0.2075, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.1795\n",
      "Epoch 18/20 Validation Accuracy: 0.7980, F1: 0.8031\n",
      "Epoch 19/20, Batch 5/16, Loss: -0.2132, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.2918\n",
      "Epoch 19/20, Batch 10/16, Loss: -0.1994, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3499\n",
      "Epoch 19/20, Batch 15/16, Loss: -0.2525, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.4952\n",
      "Epoch 19/20 Validation Accuracy: 0.8000, F1: 0.8069\n",
      "Epoch 20/20, Batch 5/16, Loss: -0.2013, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3312\n",
      "Epoch 20/20, Batch 10/16, Loss: -0.2240, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.3239\n",
      "Epoch 20/20, Batch 15/16, Loss: -0.2426, Accuracy: 1.0000, F1: 1.0000, Tau_Hat_love: -0.4007\n",
      "Epoch 20/20 Validation Accuracy: 0.8060, F1: 0.8152\n",
      "Training complete! :)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "TOP_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "print(\"TOP_DIR:\", TOP_DIR)\n",
    "if TOP_DIR not in sys.path:\n",
    "    sys.path.insert(0, TOP_DIR)\n",
    "from causalign.modules.causal_sent import CausalSent\n",
    "from causalign.data.generators import SimilarityDataset\n",
    "import wandb\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project=\"causal-sentiment\", config=args)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                    else \"mps\" if torch.backends.mps.is_available() \n",
    "                    else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "lambda_bce: float = args.lambda_bce\n",
    "lambda_reg: float = args.lambda_reg\n",
    "lambda_riesz: float = args.lambda_riesz\n",
    "batch_size: int = args.batch_size\n",
    "epochs: int = args.epochs\n",
    "log_every: int = args.log_every\n",
    "running_ate: bool = args.running_ate # whether to track a running average or batch average to compute the RR ATE\n",
    "pretrained_model_name: str = args.pretrained_model_name\n",
    "lr: float = args.lr\n",
    "estimate_targets_for_ate: bool = args.estimate_targets_for_ate # whether to use estimated sentiment probabilities or true targets to compute the RR ATE\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=True, collate_fn=SimilarityDataset.collate_fn)\n",
    "val_loader = DataLoader(ds_val, batch_size=batch_size, collate_fn=SimilarityDataset.collate_fn)\n",
    "\n",
    "# Model, optimizer, and loss\n",
    "model = CausalSent(bert_hidden_size=768, pretrained_model_name=pretrained_model_name).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    train_targets, train_predictions = [], []\n",
    "    \n",
    "    for i, batch in enumerate(train_loader):\n",
    "        input_ids_real = batch['input_ids_real'].to(device)\n",
    "        input_ids_treated = batch['input_ids_treated'].to(device)\n",
    "        input_ids_control = batch['input_ids_control'].to(device)\n",
    "        attention_mask_real = batch['attention_mask_real'].to(device)\n",
    "        attention_mask_treated = batch['attention_mask_treated'].to(device)\n",
    "        attention_mask_control = batch['attention_mask_control'].to(device)\n",
    "        targets = batch['targets'].float().to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        (sentiment_outputs_real, sentiment_outputs_treated, sentiment_outputs_control, \n",
    "        riesz_outputs_real, riesz_outputs_treated, riesz_outputs_control) = model(\n",
    "            input_ids_real,\n",
    "            input_ids_treated,\n",
    "            input_ids_control,\n",
    "            attention_mask_real,\n",
    "            attention_mask_treated,\n",
    "            attention_mask_control,\n",
    "        )\n",
    "\n",
    "        # Compute tau_hat\n",
    "        if running_ate:\n",
    "            if \"epoch_riesz_outputs\" not in locals():\n",
    "                epoch_riesz_outputs, epoch_sentiment_outputs, epoch_targets = [], [], []\n",
    "            epoch_riesz_outputs.append(riesz_outputs_real.detach())\n",
    "            epoch_sentiment_outputs.append(torch.sigmoid(sentiment_outputs_real.detach()))\n",
    "            epoch_targets.append(targets.detach())\n",
    "\n",
    "            all_riesz_outputs = torch.cat(epoch_riesz_outputs, dim=0)\n",
    "            all_sentiment_outputs = torch.cat(epoch_sentiment_outputs, dim=0)\n",
    "            all_targets = torch.cat(epoch_targets, dim=0)\n",
    "\n",
    "            tau_hat = torch.mean(all_riesz_outputs * all_sentiment_outputs if estimate_targets_for_ate else all_targets)\n",
    "        else:\n",
    "            tau_hat = torch.mean(riesz_outputs_real * (torch.sigmoid(sentiment_outputs_real) if estimate_targets_for_ate else targets))\n",
    "        \n",
    "        # Compute losses\n",
    "        riesz_loss = torch.mean(-2 * (riesz_outputs_treated - riesz_outputs_control) + (riesz_outputs_real ** 2))\n",
    "        reg_loss = torch.mean((sentiment_outputs_treated - sentiment_outputs_control - tau_hat) ** 2)\n",
    "        bce = bce_loss(sentiment_outputs_real.squeeze(), targets)\n",
    "        loss = lambda_bce * bce + lambda_reg * reg_loss + lambda_riesz * riesz_loss\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Training metrics\n",
    "        preds = torch.sigmoid(sentiment_outputs_real).squeeze().detach().cpu().numpy()\n",
    "        preds = (preds > 0.5).astype(int)\n",
    "        train_targets.extend(targets.cpu().numpy())\n",
    "        train_predictions.extend(preds)\n",
    "\n",
    "        # Logging\n",
    "        if (i + 1) % log_every == 0:\n",
    "            train_acc = accuracy_score(train_targets, train_predictions)\n",
    "            train_f1 = f1_score(train_targets, train_predictions)\n",
    "            wandb.log({\"Train Loss\": loss.item(), \n",
    "                    \"Train Accuracy\": train_acc, \n",
    "                    \"Train F1\": train_f1, \n",
    "                    f\"Tau_Hat_{args.treatment_phrase}\": tau_hat.item(),\n",
    "                    \"Batch\": i + 1})\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{epochs}, \"\n",
    "                f\"Batch {i + 1}/{len(train_loader)}, \"\n",
    "                f\"Loss: {loss.item():.4f}, \"\n",
    "                f\"Accuracy: {train_acc:.4f}, \"\n",
    "                f\"F1: {train_f1:.4f}, \"\n",
    "                f\"Tau_Hat_{args.treatment_phrase}: {tau_hat.item():.4f}\"\n",
    "            )\n",
    "    # ======= Validation Metrics =======\n",
    "    model.eval()\n",
    "    val_targets, val_predictions = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids_real = batch['input_ids_real'].to(device)\n",
    "            attention_mask_real = batch['attention_mask_real'].to(device)\n",
    "            targets = batch['targets'].float().to(device)\n",
    "            \n",
    "            sentiment_output_real = model(input_ids_real, None, None, attention_mask_real, None, None)\n",
    "            preds = torch.sigmoid(sentiment_output_real).squeeze().cpu().numpy()\n",
    "            preds = (preds > 0.5).astype(int)\n",
    "            \n",
    "            val_targets.extend(targets.cpu().numpy())\n",
    "            val_predictions.extend(preds)\n",
    "    \n",
    "    # Compute validation metrics\n",
    "    val_acc = accuracy_score(val_targets, val_predictions)\n",
    "    val_f1 = f1_score(val_targets, val_predictions)\n",
    "    wandb.log({\"Val Accuracy\": val_acc, \"Val F1\": val_f1, \"Epoch\": epoch + 1})\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} Validation Accuracy: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
    "\n",
    "print(\"Training complete! :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causalign",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
