{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replication of Bansal et al. (2023)'s ITVReg [DEPRECATED]\n",
    "\n",
    "Deprecated because the original code is a mess and it would be easier (and a better scientific validation)\n",
    "to try to re-implement their method based on the math proposed in their paper. \n",
    "\n",
    "Found the original code (messy) at https://github.com/pbansal5/causal-recommendations. Going to try to clean up and replicate results. In particular the interventions are unclear both in their description in the paper and in the code, so some liberty may need to be taken there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielfrees/miniconda3/envs/causalign/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# removing xclib requirement since it is a small library and does not support building on \n",
    "# new versions of python. Riddled with errors, wasn't able to patch all of them quickly. \n",
    "\n",
    "# nagame also removed, not even working in the OG work code lol\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import os\n",
    "import transformers as ppb\n",
    "from transformers import RobertaModel, AutoConfig\n",
    "import warnings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "warnings.filterwarnings('ignore')\n",
    "# from xclib.data.data_utils import read_sparse_file\n",
    "import copy\n",
    "from transformers import get_scheduler,AdamW\n",
    "from transformers import AutoTokenizer\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug\n",
    "import nltk\n",
    "# import xclib.evaluation.xc_metrics as xc_metrics\n",
    "# import xclib.data.data_utils as data_utils\n",
    "import scipy.sparse as sp\n",
    "# from ngame.nns import exact_search\n",
    "\n",
    "\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "torch.set_default_device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_DIR = os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 294805 training records\n",
      "Loaded 134835 testing records\n",
      "Loaded 131073 labels\n",
      "Sample training record: {'uid': 'B0007J0FJ0', 'title': 'Methodical Bible study: A new approach to hermeneutics', 'content': 'Inductive study compares related Bible texts in order to let the Bible interpret itself, rather than approaching Scripture with predetermined notions of what it will say.  Dr. Trainas Methodical Bible Study was not intended to be the last word in inductive Bible study; but since its first publication in 1952, it has become a foundational text in this field.  Christian colleges and seminaries have made it required reading for beginning Bible students, while many churches have used it for their lay Bible study groups.  Dr. Traina summarizes its success in this comment:  \"If the truths of the Bible already resided in man, there would be no need for the Bible and this manual would be superfluous.  But the fact is the Bible is an objective body of literature which exists because man needs to know certain truths which he himself cannot know.  There are two main approaches open to the Bible student.  One is deduction, which begins with generalizations and moves for their support to the particulars.  By its very nature deduction tends to be subjective and prejudicial.  Its opposite, induction, is objective and impartial; for it demands that one first examine the particulars of the Scriptures and that ones conclusions be based on those particulars.  Such an approach is sound because, being objective, it corresponds to the objective nature of the Scriptures.\"  This book fills the need for a simple, practical textbook in hermeneutics.  It encourages the serious Bible student to practice the best kind of hermeneutic, which allows the Word of God to speak for itself.\\t      --This text refers to an out of print or unavailable edition of this title.\\tRobert A. Traina was Thompson Professor of Biblical Studies at Asbury Theological Seminary, Wilmore KY.  He held the STB and STM degrees from Biblical Seminary in New York and the PhD degree from Drew Uinversity.\\t      --This text refers to an out of print or unavailable edition of this title.', 'target_ind': [4315], 'target_rel': [1.0]}\n",
      "Sample test record: {'uid': 'B0009A41VK', 'title': 'Handel: Messiah / Athalia / Esther / La Resurrezione', 'content': \"Led by Christopher Hogwood, the Academy of Ancient Music has made many renowned recordings of Handel's music-particularly the oratorios. The beloved Messiah heads up this 8-CD set, followed by Esther; La Resurezzione , and, making its return to the international catalog after an absence of several years, the 1985 recording of Athalia -with none other than Joan Sutherland in the title role! Recorded in London, 1979-85.\", 'target_ind': [42537, 43834], 'target_rel': [1.0, 1.0]}\n",
      "Sample label: {'uid': '0001055313', 'title': 'Hundred Days', 'content': '', 'target_ind': [], 'target_rel': []}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gdown\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "# Set the directory to store data (no need to set TOP_DIR)\n",
    "data_dir = 'data'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "# Step 1: Download the dataset from Google Drive using the correct link\n",
    "url = \"https://drive.google.com/uc?id=1WuquxCAg8D4lKr-eZXPv4nNw2S2lm7_E\"\n",
    "output_zip = os.path.join(data_dir, 'LF-AmazonTitles-131K.raw.zip')\n",
    "if not os.path.exists(output_zip):\n",
    "    gdown.download(url, output_zip, quiet=False)\n",
    "\n",
    "# Step 2: Extract the downloaded .zip file\n",
    "with zipfile.ZipFile(output_zip, 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_dir)\n",
    "\n",
    "# Step 3: Load the compressed JSON data from 'trn.json.gz', 'tst.json.gz', and 'lbl.json.gz'\n",
    "def load_gz_json_line_by_line(file_path):\n",
    "    data = []\n",
    "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error decoding JSON on line: {line}\")\n",
    "    return data\n",
    "\n",
    "# Paths for train, test, and label JSON data\n",
    "unzipped_dir = \"LF-Amazon-131K\"\n",
    "train_data_path = os.path.join(data_dir, unzipped_dir, 'trn.json.gz')\n",
    "test_data_path = os.path.join(data_dir, unzipped_dir, 'tst.json.gz')\n",
    "label_data_path = os.path.join(data_dir, unzipped_dir, 'lbl.json.gz')\n",
    "\n",
    "# Load the datasets line by line\n",
    "train_data = load_gz_json_line_by_line(train_data_path)\n",
    "test_data = load_gz_json_line_by_line(test_data_path)\n",
    "label_data = load_gz_json_line_by_line(label_data_path)\n",
    "\n",
    "# Step 4: Optionally load the filter label files (if needed)\n",
    "filter_train_path = os.path.join(data_dir, unzipped_dir, 'filter_labels_train.txt')\n",
    "filter_test_path = os.path.join(data_dir, unzipped_dir, 'filter_labels_test.txt')\n",
    "\n",
    "with open(filter_train_path, 'r') as f:\n",
    "    filter_train = f.readlines()\n",
    "\n",
    "with open(filter_test_path, 'r') as f:\n",
    "    filter_test = f.readlines()\n",
    "\n",
    "# Step 5: Print out some information about the loaded datasets\n",
    "print(f\"Loaded {len(train_data)} training records\")\n",
    "print(f\"Loaded {len(test_data)} testing records\")\n",
    "print(f\"Loaded {len(label_data)} labels\")\n",
    "\n",
    "# Optionally print a sample to see the structure\n",
    "if train_data:\n",
    "    print(\"Sample training record:\", train_data[0])\n",
    "if test_data:\n",
    "    print(\"Sample test record:\", test_data[0])\n",
    "if label_data:\n",
    "    print(\"Sample label:\", label_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sparse matrix shape: (294805, 131073)\n",
      "Testing sparse matrix shape: (134835, 131073)\n",
      "Training sparse matrix non-zero elements: 674996\n",
      "Testing sparse matrix non-zero elements: 284746\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Example: Using the data from earlier\n",
    "\n",
    "# Assume train_data, test_data, and label_data are already loaded from the JSON files\n",
    "# Here we'll use the 'target_ind' and 'target_rel' fields to build sparse matrices\n",
    "\n",
    "def create_sparse_matrix(data, num_rows, num_cols):\n",
    "    \"\"\"Create a sparse matrix from 'target_ind' and 'target_rel'.\"\"\"\n",
    "    row_indices = []\n",
    "    col_indices = []\n",
    "    values = []\n",
    "    \n",
    "    for row, record in enumerate(data):\n",
    "        indices = record['target_ind']\n",
    "        rels = record['target_rel']\n",
    "        row_indices.extend([row] * len(indices))\n",
    "        col_indices.extend(indices)\n",
    "        values.extend(rels)\n",
    "    \n",
    "    # Create a sparse matrix in CSR format\n",
    "    sparse_matrix = sp.csr_matrix((values, (row_indices, col_indices)), shape=(num_rows, num_cols))\n",
    "    \n",
    "    return sparse_matrix\n",
    "\n",
    "# Get the dimensions\n",
    "num_train_samples = len(train_data)\n",
    "num_test_samples = len(test_data)\n",
    "num_labels = len(label_data)  # This should be the number of columns (unique target indices)\n",
    "\n",
    "# Create the sparse matrices\n",
    "train_sparse_matrix = create_sparse_matrix(train_data, num_train_samples, num_labels)\n",
    "test_sparse_matrix = create_sparse_matrix(test_data, num_test_samples, num_labels)\n",
    "\n",
    "# Example: Print information about the created sparse matrix\n",
    "print(f\"Training sparse matrix shape: {train_sparse_matrix.shape}\")\n",
    "print(f\"Testing sparse matrix shape: {test_sparse_matrix.shape}\")\n",
    "\n",
    "# If you want to see the non-zero elements count:\n",
    "print(f\"Training sparse matrix non-zero elements: {train_sparse_matrix.nnz}\")\n",
    "print(f\"Testing sparse matrix non-zero elements: {test_sparse_matrix.nnz}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, datadir,mapping, device,split):\n",
    "        self.lbl_size = 131073\n",
    "        self.datadir = datadir\n",
    "        self.device=device\n",
    "        self.size = 294805 if split =='trn' else 134835\n",
    "        if split == 'trn':\n",
    "            self.point_text_files = [x.strip() for x in open('%s/raw/trn_X.title.txt'%self.datadir).readlines()]\n",
    "        else :\n",
    "            self.point_text_files = [x.strip() for x in open('%s/raw/tst_X.title.txt'%self.datadir).readlines()]\n",
    "        self.label_text_files = [x.strip() for x in open('%s/raw/Y.title.txt'%self.datadir).readlines()]\n",
    "        self.mat_mapping = mapping\n",
    "        self.mapping = mapping.nonzero()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "        self.maxsize=32\n",
    "        self.aug = naw.SynonymAug(aug_src='wordnet',aug_max=5)\n",
    "        # aug = naw.WordEmbsAug(model_type='word2vec',model_path = './wiki-news-300d-1M.vec')\n",
    "        temp = np.fromfile('%s/tst_filter_labels.txt'%self.datadir, sep=' ').astype(int)\n",
    "        temp = temp.reshape(-1, 2).T\n",
    "        self.tst_filter_mat = sp.coo_matrix((np.ones(temp.shape[1]), (temp[0], temp[1])), tst_X_Y.shape).tocsr()\n",
    "\n",
    "    def __getitem__(self,index,augment=True):\n",
    "        point_data,label_data = self.convert_joint(self.point_text_files[self.mapping[0][index]],self.label_text_files[self.mapping[1][index]],augment=augment)\n",
    "#         point_data = self.convert(self.point_text_files[self.mapping[0][index]],augment=augment)\n",
    "#         label_data = self.convert(self.label_text_files[self.mapping[1][index]],augment=augment)\n",
    "        return (torch.Tensor(point_data['input_ids'][0]).to(device),\n",
    "               torch.Tensor(label_data['input_ids'][0]).to(device),\n",
    "               torch.Tensor(point_data['attention_mask'][0]).to(device),\n",
    "               torch.Tensor(label_data['attention_mask'][0]).to(device))\n",
    "    \n",
    "    def convert_joint(self,textp,textl,augment=True):\n",
    "        if augment : \n",
    "            textp=self.aug.augment(textp,n=1)\n",
    "            textl=self.aug.augment(textl,n=1)\n",
    "\n",
    "        combined = textp.split(' ')+textl.split(' ')\n",
    "        split_point = int(np.random.uniform(1,len(combined)))\n",
    "        textp = ' '.join(combined[:split_point])\n",
    "        textl = ' '.join(combined[split_point:])\n",
    "        \n",
    "        return (self.tokenizer(textp,add_special_tokens = True,\n",
    "                         truncation=True,return_tensors = 'np',\n",
    "                         return_attention_mask = True,padding = 'max_length',max_length=self.maxsize),\n",
    "                self.tokenizer(textl,add_special_tokens = True,\n",
    "                         truncation=True,return_tensors = 'np',\n",
    "                         return_attention_mask = True,padding = 'max_length',max_length=self.maxsize))\n",
    "    \n",
    "    def convert(self,text,augment=True):\n",
    "        if augment : \n",
    "            text=self.aug.augment(text,n=1)\n",
    "        return self.tokenizer(text,add_special_tokens = True,\n",
    "                         truncation=True,return_tensors = 'np',\n",
    "                         return_attention_mask = True,padding = 'max_length',max_length=self.maxsize)\n",
    "    \n",
    "    def get_embeds(self,model,batch_size,device):\n",
    "        labels = self.convert(self.label_text_files,augment=False)\n",
    "        points = self.convert(self.point_text_files,augment=False)\n",
    "        num_labels = labels['input_ids'].shape[0]\n",
    "        num_points = points['input_ids'].shape[0]\n",
    "        with torch.no_grad():\n",
    "            label_embeds = []\n",
    "            for i in range(0,num_labels,batch_size):\n",
    "                label_embeds.append(model.get_embed({'input_ids':torch.LongTensor(labels['input_ids'][i:i+batch_size]).to(device),\n",
    "                                          'attention_mask':torch.LongTensor(labels['attention_mask'][i:i+batch_size]).to(device)}).cpu())\n",
    "            label_embeds = torch.cat(label_embeds,dim=0)\n",
    "\n",
    "            point_embeds = []\n",
    "            for i in range(0,num_points,batch_size):\n",
    "                point_embeds.append(model.get_embed({'input_ids':torch.LongTensor(points['input_ids'][i:i+batch_size]).to(device),\n",
    "                                          'attention_mask':torch.LongTensor(points['attention_mask'][i:i+batch_size]).to(device)}).cpu())\n",
    "            point_embeds = torch.cat(point_embeds,dim=0)\n",
    "        return point_embeds,label_embeds\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mapping[0])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    point_ids,label_ids,point_masks,label_masks = zip(*batch)\n",
    "    return {'input_ids':torch.stack(point_ids,dim=0).long(),'attention_mask':torch.stack(point_masks,dim=0).long()},{'input_ids':torch.stack(label_ids,dim=0).long(),'attention_mask':torch.stack(label_masks,dim=0).long()}\n",
    "\n",
    "\n",
    "class PrecEvaluator():\n",
    "    def __init__(self, dataset, device,batch_size):\n",
    "        self.K = 5\n",
    "        self.metric = \"P\"\n",
    "        self.dataset = dataset\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.filter_mat = dataset.tst_filter_mat\n",
    "        self.best_score = -9999999\n",
    "\n",
    "    def __call__(self,model):\n",
    "        xembs,yembs = self.dataset.get_embeds(model,self.batch_size,self.device)\n",
    "        torch.cuda.empty_cache()\n",
    "        es = exact_search({'data': yembs.cpu().numpy(), 'query': xembs.cpu().numpy(), 'K': 100, 'device': self.device})\n",
    "        score_mat = es.getnns_gpu()\n",
    "        if self.filter_mat is not None:\n",
    "            self._filter(score_mat)\n",
    "        res = self.printacc(score_mat, X_Y=self.dataset.mat_mapping, K=self.K)\n",
    "        recall = xc_metrics.recall(score_mat, self.dataset.mat_mapping, k=100)*100\n",
    "        print(f'Recall@100: {\"%.2f\"%recall[99]}')        \n",
    "        score = res[str(self.K)][self.metric]\n",
    "        return score\n",
    "    \n",
    "    def _filter(self,score_mat):\n",
    "        temp = self.filter_mat.tocoo()\n",
    "        score_mat[temp.row, temp.col] = 0\n",
    "        del temp\n",
    "        score_mat = score_mat.tocsr()\n",
    "        score_mat.eliminate_zeros()\n",
    "        return score_mat\n",
    "\n",
    "            \n",
    "    def printacc(self,score_mat, K = 5, X_Y = None, disp = True):\n",
    "        if X_Y is None: X_Y = tst_X_Y\n",
    "\n",
    "        acc = xc_metrics.Metrics(X_Y.tocsr().astype(np.bool), None)\n",
    "        metrics = np.array(acc.eval(score_mat, K))*100\n",
    "        df = pd.DataFrame(metrics)\n",
    "\n",
    "        df.index = ['P', 'nDCG']\n",
    "        \n",
    "        df.columns = [str(i+1) for i in range(K)]\n",
    "        if disp: display(df.round(2))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTModel(torch.nn.Module):\n",
    "    def __init__(self,gamma):\n",
    "        super().__init__()\n",
    "        self.encoder = SentenceTransformer('msmarco-distilbert-base-v3')\n",
    "        self.rep_dim = self.encoder.get_sentence_embedding_dimension()\n",
    "        self.hidden_dim = 1024\n",
    "        self.target_encoder = copy.deepcopy(self.encoder).requires_grad_(False)\n",
    "        self.predictor = torch.nn.Sequential(torch.nn.Linear(self.rep_dim,self.hidden_dim),\n",
    "                                             torch.nn.ReLU(),torch.nn.Linear(self.hidden_dim,self.rep_dim))\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self,x,y):\n",
    "        flip = np.random.binomial(1,0.5)\n",
    "        if (flip == 0):\n",
    "            x_embeds = torch.nn.functional.normalize(self.predictor(self.encoder(x)['sentence_embedding']),dim=-1)\n",
    "            y_embeds = torch.nn.functional.normalize(self.target_encoder(y)['sentence_embedding'],dim=-1).clone().detach()\n",
    "        else:\n",
    "            y_embeds = torch.nn.functional.normalize(self.predictor(self.encoder(y)['sentence_embedding']),dim=-1)\n",
    "            x_embeds = torch.nn.functional.normalize(self.target_encoder(x)['sentence_embedding'],dim=-1).clone().detach()\n",
    "        scores = x_embeds@y_embeds.T\n",
    "        positives = torch.diag(scores).sum()\n",
    "        negatives = scores.sum()-positives\n",
    "        positives /= scores.shape[0]\n",
    "        negatives /= ((scores.shape[0]-1)*(scores.shape[0]))\n",
    "        self.update_target()\n",
    "        return positives,negatives\n",
    "    \n",
    "    def get_embed(self,x):\n",
    "        return torch.nn.functional.normalize(self.encoder(x)['sentence_embedding'],dim=-1)\n",
    "        \n",
    "    \n",
    "    def update_target(self):\n",
    "        target_dict = self.target_encoder.state_dict()\n",
    "        online_dict = self.encoder.state_dict()\n",
    "        for key in online_dict.keys():\n",
    "            target_dict[key] = target_dict[key]*self.gamma + online_dict[key]*(1-self.gamma)\n",
    "        self.target_encoder.load_state_dict(target_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/danielfrees/Desktop/causalign/data/LF-Amazon-131K/raw/trn_X.title.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m results_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{TOP_DIR}\u001b[39;00m\u001b[38;5;124m/logs/tensorboard/\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124mgamma_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124mbs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39m(expname,gamma,batch_size)\n\u001b[1;32m     14\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter(log_dir\u001b[38;5;241m=\u001b[39mresults_dir)\n\u001b[0;32m---> 17\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\u001b[43mBertDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatadir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_data_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrn_X_Y\u001b[49m\u001b[43m,\u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m,batch_size\u001b[38;5;241m=\u001b[39mbatch_size,drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn)\n\u001b[1;32m     18\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(BertDataset(datadir\u001b[38;5;241m=\u001b[39mfull_data_dir,mapping\u001b[38;5;241m=\u001b[39mtst_X_Y,split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtst\u001b[39m\u001b[38;5;124m'\u001b[39m,device\u001b[38;5;241m=\u001b[39mdevice),batch_size\u001b[38;5;241m=\u001b[39mbatch_size,drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn)\n\u001b[1;32m     20\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m,weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m, in \u001b[0;36mBertDataset.__init__\u001b[0;34m(self, datadir, mapping, device, split)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m294805\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrn\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m134835\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrn\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoint_text_files \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m/raw/trn_X.title.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatadir\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreadlines()]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m :\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoint_text_files \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m/raw/tst_X.title.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatadir)\u001b[38;5;241m.\u001b[39mreadlines()]\n",
      "File \u001b[0;32m~/miniconda3/envs/causalign/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/danielfrees/Desktop/causalign/data/LF-Amazon-131K/raw/trn_X.title.txt'"
     ]
    }
   ],
   "source": [
    "expname = 'WordNetAugCut'\n",
    "unzipped_dir = \"LF-Amazon-131K\"\n",
    "full_data_dir = os.path.join(TOP_DIR, data_dir, unzipped_dir)\n",
    "trn_X_Y = train_sparse_matrix\n",
    "tst_X_Y = test_sparse_matrix\n",
    "device = torch.device('mps')\n",
    "batch_size = 1024\n",
    "max_epoch = 500\n",
    "gamma = 0.995\n",
    "iteration = 0\n",
    "\n",
    "model = BERTModel(gamma = gamma).to(device)\n",
    "results_dir = f'{TOP_DIR}/logs/tensorboard/{expname}_gamma-{gamma}_bs-{batch_size}\n",
    "writer = SummaryWriter(log_dir=results_dir)\n",
    "\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(BertDataset(datadir=full_data_dir,mapping=trn_X_Y,split='trn',device=device),batch_size=batch_size,drop_last=True,shuffle=True,collate_fn=collate_fn)\n",
    "test_dataloader = torch.utils.data.DataLoader(BertDataset(datadir=full_data_dir,mapping=tst_X_Y,split='tst',device=device),batch_size=batch_size,drop_last=True,shuffle=True,collate_fn=collate_fn)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4,weight_decay=1e-6)\n",
    "num_training_steps = max_epoch * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=1000,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "evaluator = PrecEvaluator(test_dataloader.dataset,device,100)\n",
    "best_score = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch 0 at iteration 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_epoch):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstarting epoch \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m at iteration \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m%\u001b[39m(epoch,iteration))\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _,(x,y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtrain_dataloader\u001b[49m):\n\u001b[1;32m      4\u001b[0m         positives,negatives \u001b[38;5;241m=\u001b[39m model(x,y)\n\u001b[1;32m      5\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mpositives\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epoch):\n",
    "    print (\"starting epoch %d at iteration %d\"%(epoch,iteration))\n",
    "    for _,(x,y) in enumerate(train_dataloader):\n",
    "        positives,negatives = model(x,y)\n",
    "        loss = -positives\n",
    "        exit()\n",
    "        if (iteration %100  == 0):\n",
    "            writer.add_scalar('train/positives',positives,iteration)\n",
    "            writer.add_scalar('train/negatives',negatives,iteration)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        iteration += 1\n",
    "    if(epoch %1 == 0):\n",
    "        positives,negatives,count = 0,0,0\n",
    "        with torch.no_grad():\n",
    "            for _,(x,y) in enumerate(test_dataloader):\n",
    "                positives_,negatives_ = model(x,y)\n",
    "                positives += positives_\n",
    "                negatives += negatives_\n",
    "                count += 1\n",
    "        score = evaluator.__call__(model)\n",
    "        writer.add_scalar('val/P@5',score,iteration)\n",
    "        writer.add_scalar('val/positives',positives/count,iteration)\n",
    "        writer.add_scalar('val/negatives',negatives/count,iteration)\n",
    "        if (score < best_score):\n",
    "            best_score = score\n",
    "            torch.save(model.state_dict(),os.path.join(results_dir,'checkpoint.pt'))\n",
    "            print ('saved checkpoint for epoch %d'%epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causalign",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
